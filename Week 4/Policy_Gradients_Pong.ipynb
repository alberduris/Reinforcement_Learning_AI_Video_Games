{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients algorithm for Pong\n",
    "\n",
    "Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym.\n",
    "\n",
    "Original source: [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*H*: Number of hidden layers neurons<br>\n",
    "*batch_size*: Number of episodes to do param update<br>\n",
    "*gamma*: Discount factor for reward<br>\n",
    "*decay_rate*: Decay factor for optimizer<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = 200\n",
    "batch_size = 10 \n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 \n",
    "decay_rate = 0.99\n",
    "\n",
    "resume = False\n",
    "render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    '''\n",
    "    Initialization of the neural network model\n",
    "    '''\n",
    "    D = 80 * 80 #Input dimen\n",
    "    if resume:\n",
    "        pass\n",
    "    else:\n",
    "        model = {}\n",
    "        model['W1'] = np.random.randn(H,D) / np.sqrt(D) #\"Xavier\" init\n",
    "        model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "\n",
    "    #Update buffers that add up gradients over a batch\n",
    "    grad_buffer = { k : np.zeros_like(v) for k,v in model.items()}  \n",
    "    #Rmsprop memory\n",
    "    rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items()}\n",
    "    \n",
    "    return model, grad_buffer, rmsprop_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    '''\n",
    "    Preprocess 210x160x3 uint8 frame into (80x80) 1D float vector\n",
    "    '''\n",
    "    I = I[35:195] #Crop\n",
    "    I = I[::2,::2,0] #Downsample by factor of 2\n",
    "    I[I==144] = 0 #Erase background type 1\n",
    "    I[I==109] = 0 #Erase background type 2\n",
    "    I[I != 0] = 1 #Everything else set to 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_reward(r):\n",
    "    '''\n",
    "    Take 1D float array of rewards and compute discounted reward\n",
    "    '''\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0,r.size)):\n",
    "        if r[t] != 0: running_add = 0 #Reset the sum, since this was a game boundary\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_forward(x):\n",
    "    h = np.dot(model['W1'],x)\n",
    "    h[h<0] = 0 #ReLU\n",
    "    logp = np.dot(model['W2'],h)\n",
    "    p = sigmoid(logp)\n",
    "    return p,h #Return prob and hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_backward(eph, epdlogp):\n",
    "    '''\n",
    "    Backward pass (eph is array of intermediate hidden states)\n",
    "    '''\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "    dh = np.outer(epdlogp, model['W2'])\n",
    "    dh[eph <= 0] = 0 #Backprop prelu\n",
    "    dW1 = np.dot(dh.T, epx)\n",
    "    return {'W1':dW1, 'W2': dW2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    env = gym.make('Pong-v0') #No module\n",
    "    observation = env.reset()\n",
    "    prev_x = None #Computing the difference frame\n",
    "    xs,hs,dlogps,drs = [],[],[],[]\n",
    "    running_reward = None\n",
    "    reward_sum = 0\n",
    "    episode_number = 0\n",
    "    while True:\n",
    "        if render: env.render()\n",
    "\n",
    "        #Preprocess the observation, set input to network to be difference image\n",
    "        cur_x = prepro(observation)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "        prev_x = cur_x\n",
    "\n",
    "        #Forward the policy network and sample action from the returned prob\n",
    "        aprob, h = policy_forward(x)\n",
    "        action = 2 if np.random.uniform() < aprob else 3 #roll the dice!\n",
    "\n",
    "        #Record various intermediates (needed later for backprop)\n",
    "        xs.append(x) #Observation\n",
    "        hs.append(h) #Hidden state\n",
    "        y = 1 if action == 2 else 0 #A 'fake label'\n",
    "        dlogps.append(y - aprob) #Grad that encourages the action that was taken to be taken\n",
    "\n",
    "        #Step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        drs.append(reward) # Record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done:\n",
    "            episode_number += 1\n",
    "\n",
    "            #Stack together all inputs, hs, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            eph = np.vstack(hs)\n",
    "            epdlogp = np.vstack(dlogps)\n",
    "            epr = np.vstack(drs)\n",
    "            xs,hs,dlogps,drs = [],[],[],[]\n",
    "\n",
    "            #Compute the discounted reward backwards through time\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            #Standardize the rewards to be unit normal\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "            #Policy gradient magic\n",
    "            epdlogp *= discount_reward # Modulate the gradient with advantage\n",
    "            grad = policy_backward(eph, epdlogp)\n",
    "            for k in model: grad_buffer[k] += grad[k] #Accumulate grad over batch\n",
    "\n",
    "            #Perform rmsprop parameter update every batch_size episodes\n",
    "            if episode_number % batch_size == 0:\n",
    "                for k,v in model.items():\n",
    "                    g = grad_buffer[k] #Gradient\n",
    "                    rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "                    model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "                    grad_buffer[k] = np.zeros_like(v) #Reset batch gradient buffer\n",
    "\n",
    "            #Boring book-keeping\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print('Resetting env. episode reward total was %.3f. running mean: %.3f' % (reward_sum, running_reward))\n",
    "            if episode_number % 100 == 0: pickle.dump(model, open('save.p','wb'))\n",
    "            reward_sum = 0\n",
    "            observation = env.reset()\n",
    "            prev_x = None\n",
    "\n",
    "        if reward != 0: #Pong has either +1 or -1 reward exactly when game ends\n",
    "            print('Ep %d: game finished, reward: %.f' % (episode_number,reward)) + ('' if reward == -1 else ' !!!!!!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning\n",
    "\n",
    "Pong environment not available on Windows (12/10/17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)\n",
    "2. [Simple Reinforcement Learning with Tensorflow: Part 2 - Policy-based Agents](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724)\n",
    "3. [Reinforcement learning with policy gradient](http://minpy.readthedocs.io/en/latest/tutorial/rl_policy_gradient_tutorial/rl_policy_gradient.html)\n",
    "4. [Deep Deterministic Policy Gradients in TensorFlow](http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html)\n",
    "5. [Simple reinforcement learning methods to learn CartPole](http://kvfrans.com/simple-algoritms-for-solving-cartpole/)\n",
    "6. [REINFORCEMENT LEARNING (RL) â€“ POLICY GRADIENTS I](https://theneuralperspective.com/2016/11/25/reinforcement-learning-rl-policy-gradients-i/)\n",
    "7. [Solving the Basic Game of Pong](https://www.youtube.com/watch?v=pN7ETkOizGM&ab_channel=SirajRaval)\n",
    "8. [Pong-v0](https://gym.openai.com/envs/Pong-v0/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
